<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Big picture &#8212; Pseudorandomness Spring 2017 documentation</title>
    
    <link rel="stylesheet" href="../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     'Spring 2017',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body role="document">
  <div class="document">
    
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper"><h3>Navigation</h3>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../seminar/index.html">Seminars</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html">Groups</a></li>
<li class="toctree-l1"><a class="reference internal" href="../open_problems/index.html">Open Problems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reading/index.html">Reading Lists</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html">References</a></li>
</ul>


<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="../search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="big-picture">
<h1>Big picture<a class="headerlink" href="#big-picture" title="Permalink to this headline">¶</a></h1>
<div class="line-block">
<div class="line">We’ll talk about several results which have different names in
different fields. You probably know them, but don’t know the same or
related idea comes up in the other fields.</div>
</div>
<p>|c|c|c|c|c|c| &amp; Boosting &amp; Hard-core lemma &amp; Dense model theorem
&amp; Weak regularity &amp; ? Area &amp; ML &amp; CC, Derandom-ization &amp; Additive
combinatorics, CC &amp; Graph theory &amp; Credit &amp; Shapiro, Freund-Schapire &amp;
Impagliazzo, Holenstein &amp; Green-Tao, Barak-Shaltiel-Wigderson &amp;
Szemeredi, Frieze-Kannan &amp; Get &amp; Circuit computing <span class="math">\(f\)</span>
<span class="math">\(1-\delta\)</span> of the time &amp; ” &amp; Proof that set isn’t
<span class="math">\(\delta\)</span>-dense &amp; ” &amp; Unless &amp; Weak learner fails on distribution
of density <span class="math">\(\Omega(\delta)\)</span> &amp; Hard-core distribution &amp;
<span class="math">\(\Omega(\de)\)</span>-dense “model” indistinguishable from set &amp; A model
succinctly describing set &amp; Algorithm needed &amp; Weak learner &amp; ” &amp;
Distinguisher &amp; ” &amp;</p>
<p>We will take these theorems that we know to be true and show
implications between them. Implications are due to...</p>
<ol class="arabic simple">
<li>Boosting<span class="math">\(\implies\)</span>Hard-core: Klivans and Servedio.</li>
<li>Hard-core<span class="math">\(\implies\)</span>Dense model: Impagliazzo</li>
<li>Dense model<span class="math">\(\implies\)</span>Weak regularity:
Trevisan-Tulsiani-Vadhan, Reingold-Trevisan-Tulsiani-Vadhan</li>
<li>Weak regularity<span class="math">\(\implies\)</span>boosting: Trevisan-Tulsiani-Vadhan</li>
</ol>
<ul class="simple">
<li></li>
</ul>
<p>What can we gain from looking at these connections?</p>
<ol class="arabic">
<li><p class="first">Versatility: We can “retrofit” algorithms for one setting to get
algorithms for the other settings.</p>
<p>For example, there are many boosting algorithms. When you follow this
progression, you get different quantitative and qualitative versions
of dense model theorem and regularity.</p>
</li>
<li><p class="first">Algorithmic and constructive results:</p>
<p>There are nonconstructive versions using the min-max theorem for
boosting, hard-core lemma, dense model theorem. We care about
algorithmic versions.</p>
<p>Note that the algorithmic result that we care about is different in
the different settings. In ML we care about getting a function that
computes a function much of the time. On the other side, we’re really
after the distribution where the weak learner fails, so that we get a
model that succinctly describes the set.</p>
<p>We pay attention to do the reductions in an algorithmic, not just an
existential way.</p>
</li>
<li><p class="first">Using the dense model theorem for learning. Can we take a boosting
technique and use it in an unsupervised way?</p>
</li>
<li><p class="first">Generality: some things seem to be specific to a setting (density of
graphs).</p>
<p>But actually, weak regularity doesn’t have anything to do with graphs
being dense. We can relativize it to subgraphs of any graph. You can
look at subgraphs of expanders, bipartite graphs, etc., and plug it
in the same machinery. Likewise if you want to look at spectral norms
rather than cuts.</p>
</li>
</ol>
<p>Here is a cartoon:</p>
<ol class="arabic">
<li><p class="first">Let <span class="math">\(X\)</span> be a set, e.g. a distribution of points in the square.
Let <span class="math">\(S\)</span> be some distribution on points in <span class="math">\(X\)</span>.</p>
<p>Let <span class="math">\(\mathcal T\)</span> be a set of classifiers, ex. a set of
half-planes.</p>
<p>Let <span class="math">\(\mathcal F_K\mathcal T\)</span> be boolean functions on <span class="math">\(K\)</span>
functions in <span class="math">\(\mathcal T\)</span>; here, partitions into polygonal
regions by <span class="math">\(k\)</span> half-planes.</p>
<p>We want to pre-process the distribution to be able to answer queries
in <span class="math">\(\mathcal F_K\mathcal T\)</span>.</p>
</li>
<li><p class="first">A violation of pseudo-density in this setting means there is a
polygonal region with many more points from <span class="math">\(S\)</span> than its
volume, a “hot spot”.</p>
<div class="math">
\[\text{Area}\pat{region}&lt; \de \Pj_S\pat{region}-\ep.\]</div>
</li>
<li><p class="first">A model is a partition into polygonal regions, with a probability
distribution on regions. A simple model is defined by at most
<span class="math">\(k\)</span> lines.</p>
<p>The property of a model is that we can estimate half-space
probabilities (“given any half-space, what proportion of points of
<span class="math">\(S\)</span> are on one side of it?”) by treating the points as if
uniform within regions.</p>
</li>
<li><p class="first">The algorithmic requirement in order to process the points to answer
queries in <span class="math">\(\mathcal F_K\mathcal T\)</span> is: given a set of points
sampled from <span class="math">\(S\)</span>, and a set of points sampled from <span class="math">\(U\)</span>,
find a half-space that approximately maximizes the difference in
probabilities for these two sets. The equivalent in boosting is a
distinguishing algorithm.</p>
</li>
</ol>
<p>|p20mm|p35mm|p35mm|p35mm|p15mm| Setting &amp; Boosting &amp; Hard-core
measure &amp; DMT/transference principle &amp; Weak regularity</p>
<p>&amp; WL: <span class="math">\(|\mu_{i}|\ge2\delta\)</span>,
<span class="math">\(\mu_{i}=g(h_{1,}\ldots,h_{i},f)\)</span>, <span class="math">\(h_{i+1}\in\mathcal{T}\)</span>,
<span class="math">\(k\)</span> iterations &amp; Hardcore measure:
<span class="math">\(\mu_{k}=g(h_{1},\ldots,h_{k},f)\)</span>, <span class="math">\(|\mu_{k}|\ge2\delta\)</span> &amp;
Model: <span class="math">\(\mu_{k}=g(h_{1},\ldots,h_{k},o)\)</span>,
<span class="math">\(|\mu_{k}|\ge\delta\)</span>&amp; &amp; SL: <span class="math">\(H=G(h_{1},\ldots,h_{k})\)</span>,
<span class="math">\(\Pj[H=f]\ge1-\delta\)</span> &amp; Violation of hardness:
<span class="math">\(H=G(h_{1},\ldots,h_{k})\)</span>, <span class="math">\(\Pj[H=f]&gt;1-\delta\)</span> &amp; Violation
of pseudo-density <span class="math">\(H=G(h_{1},\ldots,h_{k})\)</span>,
<span class="math">\(H(U)\le\delta H(S)-\ep\)</span>&amp; Assumption &amp; WL never fails &amp;
Violation is impossible &amp; Violation of pseudo-density is impossible &amp;
Actually dense Conclusion &amp; SL works &amp; Hard-core measure exists, with
same <span class="math">\(k\)</span>, <span class="math">\(G\)</span>, <span class="math">\(g\)</span> &amp; Model exists &amp; Model exists
Algorithmic &amp; Weak learner requirement &amp; Approximately optimal weak
learner &amp; Approximately optimal distinguisher &amp;</p>
<p>|p20mm|p35mm|p35mm|p35mm|p15mm| Setting &amp; Boosting &amp; Hard-core
measure &amp; DMT/transference principle &amp; Weak regularity</p>
<p>&amp; WL: <span class="math">\(|\mu_{i}|\ge2\delta\)</span>,
<span class="math">\(\mu_{i}=g(h_{1,}\ldots,h_{i},f)\)</span>, <span class="math">\(h_{i+1}\in\mathcal{T}\)</span>,
<span class="math">\(k\)</span> iterations &amp; Hardcore measure:
<span class="math">\(\mu_{k}=g(h_{1},\ldots,h_{k},f)\)</span>, <span class="math">\(|\mu_{k}|\ge2\delta\)</span> &amp;
Model: <span class="math">\(\mu_{k}=g(h_{1},\ldots,h_{k},o)\)</span>,
<span class="math">\(|\mu_{k}|\ge\delta\)</span>&amp; &amp; SL: <span class="math">\(H=G(h_{1},\ldots,h_{k})\)</span>,
<span class="math">\(\Pj[H=f]\ge1-\delta\)</span> &amp; Violation of hardness:
<span class="math">\(H=G(h_{1},\ldots,h_{k})\)</span>, <span class="math">\(\Pj[H=f]&gt;1-\delta\)</span> &amp; Violation
of pseudo-density <span class="math">\(H=G(h_{1},\ldots,h_{k})\)</span>,
<span class="math">\(H(U)\le\delta H(S)-\ep\)</span>&amp; Assumption &amp; WL never fails &amp;
Violation is impossible &amp; Violation of pseudo-density is impossible &amp;
Actually dense Conclusion &amp; SL works &amp; Hard-core measure exists, with
same <span class="math">\(k\)</span>, <span class="math">\(G\)</span>, <span class="math">\(g\)</span> &amp; Model exists &amp; Model exists
Algorithmic &amp; Weak learner requirement &amp; Approximately optimal weak
learner &amp; Approximately optimal distinguisher &amp;</p>
<p>Some comments:</p>
<ol class="arabic">
<li><p class="first">Boosting: Note it’s important that the <span class="math">\(\de\)</span> here is the same;
many boosting algorithms meet this criterion. The theorem says that
“either weak learner fails or strong learner works.”</p>
<p>In boosting, we think of weak learner as never failing.</p>
</li>
<li><p class="first">Hard-core measure lemma: The lemma says that either we can find
hard-core measure, on which no function can compute the function
<span class="math">\(f\)</span> more than <span class="math">\(\rc2+\de\)</span> of time; or find a function
computing <span class="math">\(f\)</span> <span class="math">\(1-\de\)</span> of the time.</p>
<p>Here, we want to come up with the measure. Although the logical
format is the same as boosting, here we assume that the violations
never happen (there is no strong learner). Every boosting algorithm
gives hard-core measure lemma with the same parameters, and with
exactly the same way of “gluing” the functions. Sometime you care
about computational complexity of <span class="math">\(G\)</span> but not of <span class="math">\(g\)</span>, or
vice versa.</p>
</li>
<li><p class="first">We can convert the hard-core measure theorem into the dense model
theorem/transference principle (Tao and Ziegler).</p>
<p>Here, we have a distribution we’re trying to model. Either the
distribution has pseudo-density property— there isn’t a violation
that’s definable from <span class="math">\(k\)</span> different properties from hypothesis
class, where violation means that the expected value is much smaller
on <span class="math">\(U\)</span> than on <span class="math">\(S\)</span>—or we get a model of density
<span class="math">\(\ge \de\)</span>. Assuming that violation of pseudo-density does not
happen, we get a model.</p>
</li>
<li><p class="first">Weak regularity is just DMT except the distribution actually is
dense. It’s not so interesting that it has a dense model.</p>
<p>What we get is that the dense model you get is simple, definable in
terms of a small number of basic hypotheses.</p>
<p>Sometimes we care about simplicity in the model, and sometimes
simplicity in <span class="math">\(G\)</span>.</p>
</li>
<li><p class="first">Note the <span class="math">\(k\)</span> is the same throughout. Reductions preserves
<span class="math">\(k\)</span>, and the functions <span class="math">\(h_i, G\)</span>.</p>
<p>We don’t only have the fact that boosting implies hard-core lemma
implies regularity lemma. We have the stronger result that whatever
boosting algorithm you give me, I get a hard-core lemma and
regularity lemma with the same parameters and algorithm. Thus we can
pick the boosting algorithm that gives the best results for our
application.</p>
</li>
</ol>
</div>
<div class="section" id="setup">
<h1>Setup<a class="headerlink" href="#setup" title="Permalink to this headline">¶</a></h1>
<p>First we discuss the PAC learning model.</p>
<p>Let <span class="math">\(U\)</span> be a set, and by abuse of notation, also a distribution on
that set. (Think of <span class="math">\(U\)</span> as the universe, the set of possible
inputs.) For simplicity, take the distribution to be uniform. Let
<span class="math">\(f:U\to \{0,1\}\)</span> be a boolean function. A learning algorithm can
request any number of points <span class="math">\((x,f(x))\)</span> where <span class="math">\(x\sim U\)</span>. The
goal is to find a hypothesis <span class="math">\(h\)</span> such that</p>
<div class="math">
\[\Pj_{x\sim U} [h(x)=f(x)]\ge 1-\de.\]</div>
<p>A for <span class="math">\((U,f)\)</span> with hypothesis class <span class="math">\(\mathcal H\)</span> is an
algorithm such that given samples <span class="math">\((x,f(x)), x\sim U\)</span>, outputs
<span class="math">\(h\in \mathcal H\)</span> (with high probability) such that</p>
<div class="math">
\[\Pj_{x\sim U}[h(x)=f(x)]\ge 1-\de.\]</div>
<p>(Typically, we say that the probability of success is <span class="math">\(1-\ep\)</span>, ask
for a strong learner for all <span class="math">\(f\in \mathcal F\)</span>, and require it to
run in time <span class="math">\(\poly(\rc \ep, \rc \de)\)</span>.)</p>
<p>In boosting, we assume that we have weak learners.</p>
<p>A <span class="math">\(\ep\)</span>- for <span class="math">\((\mu, f)\)</span> with hypothesis class
<span class="math">\(\mathcal H\)</span> is an algorithm such that given
<span class="math">\((x,f(x)), x\sim \mu\)</span>, outputs <span class="math">\(h\)</span> (with high probability)
such that</p>
<div class="math">
\[\Pj_{x\sim \mu} [h(x)=f(x)] \ge \rc 2+\ep.\]</div>
<p>It only has to output a function that is somewhat correlated with the
right answer. Typically, we ask the weak learner to work on any
distribution <span class="math">\(\mu\)</span> satisfying some assumptions.</p>
<p>In order to use a weak learner, we construct a routine that subsamples
the distribution <span class="math">\(U\)</span> to pass to pass to the weak learner.</p>
<p>Let <span class="math">\(\mu:U\to [0,1]\)</span>. Define the probability distribution</p>
<div class="math">
\[ \begin{align}\begin{aligned}D_\mu(x) = \fc{\mu(x)}{\sum_{x'\in U}\mu(x')}.\\[1]_\end{aligned}\end{align} \]</div>
<p>Think of this as rejection sampling: pick <span class="math">\(x\sim U\)</span>, keep it with
probability in <span class="math">\([0,1]\)</span>, or else throw if back and repeat.</p>
<p>In order for this sampling to be efficient, we need <span class="math">\(\mu\)</span> to not
be too small.</p>
<p>Define the of <span class="math">\(\mu\)</span> in <span class="math">\(U\)</span> to be</p>
<div class="math">
\[|\mu| = \EE_{x\in U} \mu(x).\]</div>
<p>We will use weak learners in the following context.</p>
<ol class="arabic">
<li><p class="first">We will only run weak learners on distributions whose density is not
too small (the dependence on <span class="math">\(\de\)</span> is <span class="math">\(|\mu|=\Om(\de)\)</span>).
We don’t want to run a weak learner on a distribution of very low
density, because the time to simulate the distribution is inversely
proportional to the density.</p>
</li>
<li><p class="first">We ask the weak learners to output a function in a given class
<span class="math">\(h\in \mathcal T\)</span>.</p>
<p>Then it will turn out that that both the measures that we run the
weak learners on, and the final hypothesis, will be describable using
<span class="math">\(\mathcal F_l \mathcal T\)</span> (see below), for some class
<span class="math">\(\mathcal F\)</span>.</p>
</li>
</ol>
<p>Say that a set <span class="math">\(\mathcal T\)</span> of functions <span class="math">\(U\to \{0,1\}\)</span> form
a class if <span class="math">\(f\in \mathcal T\)</span> implies <span class="math">\(1-f \in \mathcal T\)</span>.</p>
<p>Let <span class="math">\(\mathcal F\)</span> be a class of boolean functions. Define the class
of functions</p>
<div class="math">
\[\mathcal F_k \mathcal T = \set{f(h_1(x),\ldots, h_k(x))}{f\in\mathcal F, h_1,\ldots, h_k\in \mathcal T}.\]</div>
</div>
<div class="section" id="boosting-and-the-hard-core-lemma">
<h1>Boosting and the Hard-core lemma<a class="headerlink" href="#boosting-and-the-hard-core-lemma" title="Permalink to this headline">¶</a></h1>
<p>The first boosting algorithm we give is totally ridiculous from the ML
point of view. For people who work on weak regularity on graphs this is
the natural version, and leads to the standard versions of results.</p>
<p>We will take <span class="math">\(\mathcal F\)</span> to be the set of all boolean functions,
so given hypotheses <span class="math">\(h_1,\ldots, h_k\)</span>, we can choose the best
predictor using <span class="math">\(h_1(x),\ldots, h_k(x)\)</span>.</p>
<p>[Boosting with decision trees][thm:boosting] Let <span class="math">\(U\)</span> be a
distribution, <span class="math">\(\mathcal T\)</span> a class of boolean functions
<span class="math">\(U\to \{0,1\}\)</span>, <span class="math">\(\mathcal F\)</span> the class of all boolean
functions. Let <span class="math">\(f:U\to \{0,1\}\)</span> be a given function (which we are
trying to learn).</p>
<ol class="arabic">
<li><p class="first">Suppose that there is a <span class="math">\(\de\)</span>-weak learner such that given any
distribution <span class="math">\(\mu\)</span> on <span class="math">\(U\)</span> with <span class="math">\(|\mu|\ge 2\de\)</span>, it
produces <span class="math">\(h\in \mathcal T\)</span> such that</p>
<div class="math">
\[\Pj_{x\sim \mu} [h(x) = f(x)] \ge \rc 2+ \ep.\]</div>
</li>
<li><p class="first">Then there is a strong learner that produces
<span class="math">\(h\in \mathcal F_k\mathcal T\)</span> with
<span class="math">\(k\le\ce{\rc{\ep^2\de^2}}\)</span> such that</p>
<div class="math">
\[ \begin{align}\begin{aligned}\Pj_{x\sim U} [h(x) = f(x)]\ge 1-\de.\\[2]_\end{aligned}\end{align} \]</div>
</li>
</ol>
<p>[Hard-core lemma] [thm:hardcore] Let <span class="math">\(U\)</span> be a distribution,
<span class="math">\(\mathcal T\)</span> a class of boolean functions <span class="math">\(U\to \{0,1\}\)</span>,
<span class="math">\(\mathcal F\)</span> the class of all boolean functions.</p>
<p>Then either</p>
<ol class="arabic">
<li><p class="first">There exists <span class="math">\(h\in \mathcal F_k \mathcal T\)</span> such that</p>
<div class="math">
\[ \begin{align}\begin{aligned}\Pj_{x\sim U} [h(x)=f(x)] \ge 1-\de,\\where :math:`k\le\rc{\ep^2\de^2}`, or\end{aligned}\end{align} \]</div>
</li>
<li><p class="first">(There exists a hard-core distribution.) There exists
<span class="math">\(|\mu|\ge 2\de\)</span> on <span class="math">\(U\)</span>, such that for all
<span class="math">\(h\in \mathcal T\)</span>,</p>
<div class="math">
\[\Pj_{x\sim \mu}[h(x) = f(x)] \le \rc 2+\ep.\]</div>
</li>
</ol>
<p>Note it is important for us to keep track of the size of the hardcore
distribution, which is <span class="math">\(\ge 2\de\)</span> here. Different boosting
algorithms will give the result for different classes of functions
<span class="math">\(\mathcal F\)</span>.</p>
<p>[Proof of hard-core lemma&nbsp;[thm:hardcore] from boosting&nbsp;[thm:boosting]]
Let weak learner be exhaustive search over <span class="math">\(\mathcal T\)</span>. The weak
learner operates on distributions <span class="math">\(|\mu_i|\ge 2\de\)</span>. If it always
produces <span class="math">\(h_i\)</span> with bias <span class="math">\(\ge \de\)</span>, then continue and obtain
the strong learner: we get some <span class="math">\(H\in \mathcal F_k \mathcal T\)</span>
such that <span class="math">\(H(x)=f(x)\)</span> with probability <span class="math">\(1-\de\)</span>.</p>
<p>If at some step <span class="math">\(i\)</span> our exhaustive search algorithm gets stuck, we
get a distribution <span class="math">\(\mu_i\)</span> that’s hard-core.</p>
</div>
<div class="section" id="dense-model-theorem">
<h1>Dense model theorem<a class="headerlink" href="#dense-model-theorem" title="Permalink to this headline">¶</a></h1>
<p>For a set <span class="math">\(S\subeq U\)</span> and a function <span class="math">\(T:U \to \{0,1\}\)</span>, let
<span class="math">\(T(S):=\E_{x\in S} T(x)\)</span>. (For a measure <span class="math">\(\mu: U\to [0,1]\)</span>,
also write <span class="math">\(T(\mu) = \E_{x\sim \mu} T(x)\)</span>.)</p>
<p>Let <span class="math">\(S\subeq U\)</span> be a subset, and let <span class="math">\(\mathcal T\)</span> be a set
of tests. <span class="math">\(S\)</span> is if for all <span class="math">\(T\in \mathcal T\)</span>,</p>
<div class="math">
\[T(U) \ge \de T(S)-\ep.\]</div>
<p>Think of saying that the tests <span class="math">\(\mathcal T\)</span> don’t reveal that the
set <span class="math">\(S\)</span> is small.</p>
<ol class="arabic simple">
<li>One way of being pseudo-dense is to actually be dense.</li>
<li>Another, one step removed, is that there’s a set <span class="math">\(R\)</span> (or more
generally, a measure <span class="math">\(\mu\)</span>) that’s indistibguishable from
<span class="math">\(S\)</span> by <span class="math">\(\mathcal T\)</span>, and such that <span class="math">\(R\)</span> occupies at
least a <span class="math">\(\de\)</span> fraction of <span class="math">\(U\)</span>.</li>
</ol>
<ul class="simple">
<li></li>
</ul>
<p>For two distributions <span class="math">\(\mu_1,\mu_2\)</span> on <span class="math">\(U\)</span>, we say that
<span class="math">\(\mu_1,\mu_2\)</span> are indistinguishable by tests in <span class="math">\(\mathcal T\)</span>
up to <span class="math">\(\ep\)</span>, written <span class="math">\(\mu_1 \sim_{\mathcal T} \mu_2\)</span> within
<span class="math">\(\ep\)</span>, if for every <span class="math">\(T\in \mathcal T\)</span>,</p>
<div class="math">
\[|\E_{\mu_1}T - \E_{\mu_2} T| \le \ep.\]</div>
<p>[Dense model theorem][thm:dmt] Let <span class="math">\(\mathcal T\)</span> be a class of
tests <span class="math">\(U\to \{0,1\}\)</span>.</p>
<p>If <span class="math">\(S\)</span> is <span class="math">\((\ep,\de)\)</span>-pseudodense against
<span class="math">\(F_k\mathcal T\)</span>, <span class="math">\(k=O\prc{\ep^2\de^2}\)</span> then there exists
<span class="math">\(\mu\)</span>, <span class="math">\(\mu\in F_k\cal T\)</span> such that
<span class="math">\(|\mu|\ge \fc{\de}{1+\de}-O(\ep)\)</span> and <span class="math">\(D_\mu\sim_{\cal T} S\)</span>
to within <span class="math">\(O(\ep/\de)\)</span>.</p>
<p>The idea in the proof is to use the Hard-core lemma, with the hard
function being membership in <span class="math">\(S\)</span>.</p>
<p>Let <span class="math">\(U'\)</span> be the following distribution: let
<span class="math">\(\de'=\fc{\de}{1+\de}\)</span> and</p>
<ol class="arabic simple">
<li>with probability <span class="math">\(\de'\)</span>, take <span class="math">\(x\in S\)</span> and output
<span class="math">\((0,x)\)</span></li>
<li>with probability <span class="math">\(1-\de'\)</span>, take <span class="math">\(x\in U\)</span> and output
<span class="math">\((1,x)\)</span>.</li>
</ol>
<p>Define a test <span class="math">\(T\in \mathcal T\)</span> to operate on an example
<span class="math">\((y, x)\)</span> by <span class="math">\(T(y,x)=T(x)\)</span>. For
<span class="math">\(T\in \mathcal F_k \mathcal T\)</span>,</p>
<div class="math">
\[ \begin{align}\begin{aligned}\begin{split}  \begin{aligned}
  \Pj_{U'}[T((y,x))=y] =
  \de' T(S) + (1-\de') (1-T(U))
  &amp;= 1-\de' + \de' (T(S)) - (1-\de') T(U)\\
  &amp;= 1-\de' + \rc{1+\de} (\de T(S) - T(U))\le 1-\de'+\ep.\end{aligned}\end{split}\\No test in :math:`\mathcal F_k \mathcal T` can be correct with\end{aligned}\end{align} \]</div>
<p>probability <span class="math">\(&gt;\de'-\ep\)</span>. By the Hard-core Lemma&nbsp;[thm:hardcore],
there exists <span class="math">\(|\mu'|\ge 2(\de'-\ep)\)</span> such that for any
<span class="math">\(T\in \mathcal T\)</span>,
<span class="math">\(\Pj_{(x,y) \sim U'}[T(x)=y]\le  \rc2+\ep\)</span>.</p>
<p>In order for <span class="math">\(\mu'\)</span> to be hardcore, it must be split approximately
evenly between <span class="math">\(U\)</span> and <span class="math">\(S\)</span> (up to <span class="math">\(\ep\)</span>); otherwise;
we could have an advantage by predicting constant 0 or 1. Thus each part
has at least
<span class="math">\(2(\de'-\ep) \pa{\rc 2 - \ep} = \de'\pa{1-O\pf{\ep}{\de}}\)</span> of the
mass. Then</p>
<div class="math">
\[D_{\mu'|_U}\sim_{O(\ep)} D_{\mu'|_S}\sim_{O\pf{\ep}{\de}} S.\]</div>
</div>
<div class="section" id="proof-for-boosting">
<h1>Proof for boosting<a class="headerlink" href="#proof-for-boosting" title="Permalink to this headline">¶</a></h1>
<p>[Proof of Theorem&nbsp;[thm:boosting]] The algorithm is as follows. Let
<span class="math">\(WL(\mu)\)</span> denote the weak learner operating on <span class="math">\((\mu, f)\)</span>.</p>
<p>Let <span class="math">\(\mu_0\)</span> be constant 1, <span class="math">\(i=0\)</span>.</p>
<p>While <span class="math">\(|\mu_i|\ge 2\de\)</span>, do</p>
<ul>
<li><p class="first"><span class="math">\(h_{i+1}\mapsfrom WL(\mu_i)\)</span>.</p>
</li>
<li><p class="first">Partition <span class="math">\(U\)</span> according to values of <span class="math">\(h_1,\ldots,h_i\)</span>.</p>
<p>Let <span class="math">\(h_{1:i}(x):= (h_1(x),\ldots, h_i(x))\in \{0,1\}^i\)</span>, and
let <span class="math">\(B_i(x)\)</span> be the “block” that <span class="math">\(x\)</span> is in,</p>
<div class="math">
\[ \begin{align}\begin{aligned}B_i(x) = h_{1:i}^{-1}(h_{1:i}(x)) = \set{y\in U}{h_{1:i}(x)=h_{1:i}(y)}.\\For a set :math:`B`, let :math:`\Maj(B)` denote the majority value\end{aligned}\end{align} \]</div>
<p>of <span class="math">\(f\)</span> on <span class="math">\(B\)</span>.</p>
</li>
<li><p class="first">Define <span class="math">\(\mu_{i+1}\)</span> by</p>
<div class="math">
\[ \begin{align}\begin{aligned}\begin{split}  \mu_{i+1}(x) = \begin{cases}
  \fc{1-p_{\Maj, B_i(x)}}{p_{\Maj, B_i(x)}},&amp;\text{if } f(x)=\Maj(B_i(x))\\
  1,&amp;\text{otherwise}
  \end{cases}•\end{split}\\where :math:`p_{\Maj,B} = \Pj(f(y) = \Maj(B)| y\in B)`, the\end{aligned}\end{align} \]</div>
<p>proportion of the majority in <span class="math">\(B\)</span>.</p>
</li>
<li><p class="first"><span class="math">\(i\mapsfrom i+1\)</span>.</p>
</li>
</ul>
<p>Finally, return <span class="math">\(H_{i}(x) = \Maj({B_{i}(x)})\)</span>, i.e., look at the
block that <span class="math">\(x\)</span> is in, and choose the majority value.</p>
<p>Note that the measure <span class="math">\(\mu_{i+1}\)</span> rebalances each block
<span class="math">\(B_i\)</span> such that conditioned on <span class="math">\(y\)</span> being in a block
<span class="math">\(B_i(x)\)</span>,</p>
<div class="math">
\[ \begin{align}\begin{aligned}\Pj_{y\sim \mu_{i+1}}(f(y)=1|y\in B_i(x)) = \Pj_{y \sim \mu_{i+1}}(f(y)=0|y\in B_i(x))=\rc 2.\\Indeed, we have\end{aligned}\end{align} \]</div>
<div class="math">
\[\begin{split}\begin{aligned}
\EE_{y\sim U}[\one_{f(y)=1}
%\sum_{y\in B_i(x), f(y)=1}
 \mu_{i+1}(y) |y\in B_i(x)]
 &amp;=p_{\Maj, B_i(x)} \fc{1-p_{\Maj, B_i(x)}}{p_{\Maj, B_i(x)}} = 1-p_{\Maj, B_i(x)}\\
 \EE_{y\sim U}[\one_{f(y)=0}
%\sum_{y\in B_i(x), f(y)=1}
 \mu_{i+1}(y) |y\in B_i(x)]
 &amp;=\pa{1-p_{\Maj, B_i(x)}}\cdot 1 = 1-p_{\Maj, B_i(x)}\\
|\mu_{i+1}| =  \EE_{y\sim U}[
 \mu_{i+1}(y)]&amp;=\sum_{\text{block }B_i} [2(1-p_{\Maj, B_i}) \Pj(B_i)]\\
 &amp; \ge 2(1-p_{\Maj,U}).\end{aligned}\end{split}\]</div>
<p>Note that if <span class="math">\(|\mu_{i+1}|\le 2\de\)</span>, then
<span class="math">\(\Pj_{x\in X}[H_i=f]\ge 1-\de\)</span>, and we are done. (We stop before
we have to apply the weak learner to a distribution of density
<span class="math">\(&lt;\de\)</span>.)</p>
<p>We need to show this method terminates in a bounded number of steps.</p>
<p>Consider the potential function</p>
<div class="math">
\[ \begin{align}\begin{aligned}  \ph_i = \E_{x\sim U} [(\Pj[f=1|B_i(x)])^2]
   = \E_{x\sim U} [\E[ f|B_i]^2]\\(Think of :math:`B_i` as a partition; for a partition, :math:`\E[f|P]`\end{aligned}\end{align} \]</div>
<p>is a function of <span class="math">\(x\)</span> that takes <span class="math">\(x\)</span> to the average value in
the atom of the partition that contains <span class="math">\(x\)</span>.) Note this have value
in <span class="math">\([0,1]\)</span> and is maximized if <span class="math">\(f\)</span> is constant on every
block. We show every iteration increases this potential function by at
least a fixed amount, <span class="math">\((\ep\de)^2\)</span>. Fix a block <span class="math">\(B\)</span> in the
partition. Define <span class="math">\(p,q,\al_+,\al_-,p_0,p_1\)</span> as follows.</p>
<div class="math">
\[\begin{split}\begin{aligned}
p&amp;=\Pj[f=1|B]\\
q&amp;=\Pj[h_{i+1} = 1|B]\\ %split into 2 halves, q overall prob next fun is 1 on block
q+\al_+ &amp;= \Pj[h_{i+1}=1|B, f=1]\\ %correlated with $f$. slightly higher
q-\al_- &amp;= \Pj[h_{i+1}=1|B, f=0]\\ %cond on 0, slightly lower
%on some blocks it could flip. On average &gt;0
\al_+p &amp;= \al_-(1-p) \text{ by conservation}\\
%B&amp;\mapsfrom \al_-(1-p)\\
%B_i&amp;=B\cap \{h=i\}\\
p_0&amp;= \Pj[f=1|h=0,B] = \fc{\Pj[f=1\wedge h=0|B]}{\Pj[h=0|B]} = \fc{p(1-q-\al_+)}{1-q}\\
p_1&amp;= \Pj[f=1|h=1,B] = \fc{\Pj[f=1\wedge h=1|B]}{\Pj[h=1|B]} = \fc{p(q+\al_+)}{q}\\
\E_{x\in B} [\E[f|B_{i+1}]^2]&amp;=qp_1^2 + (1-q)p_0^2 = p^2 \pa{\fc{(q+\al_+)^2}{q} + \fc{(1-q-\al_+)^2}{1-q}} \\
&amp;=p^2\pa{
\pa{q+2\al_+ + \fc{\al_+^2}{q}}
+
\pa{1-q-2\al_+ + \fc{\al_+^2}{1-q}}
}
\\
&amp;=p^2\pa{1+\fc{\al_+^2}{q} + \fc{\al_+^2}{1-q}}\\
&amp;\ge p^2 + 4p^2 \al_+^2 \ge
p^2+
 \al_+^2\\
 %multiplicative increase by $1+\al$?
% \EE_{x\sim U} [
\E[f|B_{i+1}]^2 - \E[f|B_i]^2%]
&amp;=\al_+^2(B_i(x)).\end{aligned}\end{split}\]</div>
<p>Assume WLOG that <span class="math">\(\Maj(B_i(x))=1\)</span>. (Otherwise the LHS is smaller.)</p>
<div class="math">
\[\begin{split}\begin{aligned}
\EE_{x\in B}[\mu(x) ((-1)^{(h(x)\ne f(x))})]
&amp;=\quad p\pf{1-p}{p} \ba{(q+\al_+) - (1-q-\al_+)}&amp;(f=1)\\
&amp;\quad +(1-p) 1 [1-(1-\al_-) - (q-\al_-)]&amp;(f=0)\\
&amp;=(1-p) (2\al_++2\al_-)\\
&amp;= 2\al_+(1-p) + 2\al_+p=2\al_+\\
\EE_{x\sim U} 2\al_+(B_i(x))
&amp;=\EE_{x\sim U} [\mu(x) ((-1)^{h(x)\ne f(x)})]\\
&amp;\ge \ep|\mu| \ge 2\de \ep\\
\ph_{i+1}-\ph_i &amp;\ge
\EE_{x\sim U} [\E[f|B_{i+1}]^2 - \E[f|B_i]^2]\\
&amp;\ge
\EE_{x\sim U} \al_+^2(B_i(x))\ge  (\de\ep)^2.\end{aligned}\end{split}\]</div>
<p>• Because <span class="math">\(\ph_i\)</span> is always in <span class="math">\([0,1]\)</span>, the number of
iterations is at most <span class="math">\(k\le (\de \ep)^2\)</span>.</p>
</div>
<div class="section" id="comments-regularity-lemmas">
<h1>Comments, Regularity lemmas<a class="headerlink" href="#comments-regularity-lemmas" title="Permalink to this headline">¶</a></h1>
<p>Some comments:</p>
<ol class="arabic">
<li><p class="first">All you get from this proof is a decision tree; the complexity is
exponential in <span class="math">\(k\)</span>. This is a bug, not a feature.</p>
<p>In complexity terms, we don’t get good hard-core measure, because the
circuit size for the outer function <span class="math">\(G\)</span> is <span class="math">\(2^k\)</span>. A
better boosting algorithm would give <span class="math">\(G\)</span> have smaller
complexity. If your stopping point is the hard-core lemma, this is
not the boosting algorithm you want. For the dense model theorem,
this is fine because all you care about is size of <span class="math">\(k\)</span>, not the
complexity of <span class="math">\(G\)</span>.</p>
<p>There is another boosting algorithm which gives a weighted majority
function, which is a simpler function. A weighted majority can be
converted into a decision tree, but not vice versa.</p>
</li>
<li><p class="first">This potential function matches this boosting algorithm. Other
boosting algorithms can be analyzed with other potential functions.
This is like the potential function used most in graph theory. Key
property: you can’t make negative progress; you always go forwards.</p>
</li>
<li><p class="first">For Szemeredi regularity, we need a stronger boosting theorem.
Suppose we get stuck at some step: no function correlates globally,
but there are many blocks where we can find functions that correlate
with the function inside that block. If in <span class="math">\(\ep\)</span> fraction of
blocks we find functions that correlate, partition them based on all
the values of these functions, and repeat.</p>
<p>In one step we’ve gone from order of <span class="math">\(2^k\)</span> to order of
<span class="math">\(2^{2^k}\)</span> buckets, and increased the potential function by a
polynomial in terms of <span class="math">\(\ep,\de\)</span>. This is a familiar argument;
we can only go <span class="math">\(\rc{\ep}\)</span> iterations before we terminate. This
time, the number of sets is a tower depending on <span class="math">\(\ep\)</span>.</p>
</li>
<li><p class="first">Regularity lemmas:</p>
<p>Fix a set of vertices <span class="math">\(V\)</span> of set <span class="math">\(n\)</span>. Let <span class="math">\(U\)</span> be
edges in complete graph on <span class="math">\(V\)</span>. (We can also consider the case
when <span class="math">\(U\)</span> is not the complete graph, ex. <span class="math">\(U\)</span> is the edges
in <span class="math">\(d\)</span>-regular expander on <span class="math">\(V\)</span>.)</p>
<p>The underlying set we care about is the set of cuts defined by
<span class="math">\(A,B\subeq V\)</span> where <span class="math">\(A\cap B=\phi\)</span>; there are <span class="math">\(3^k\)</span>
of them.</p>
<p>If <span class="math">\(|E|\ge \de \binom n2\)</span>, the generic regularity lemma says
there exists <span class="math">\(\mu=G(T_1,\ldots, T_k)\)</span>, where
<span class="math">\(k=O\prc{\ep^2\de^2}\)</span>, that is a good predictor the number of
edges of any cut in the graph. Use the <span class="math">\(T\)</span>’s to divide the
vertices into <span class="math">\(3^k\)</span> subsets such that <span class="math">\(\mu\)</span> is a constant
on every pair of subsets.</p>
<div class="math">
\[\fc{E_G(A,B)}{|E_G|}
\approx_\ep \sum_{i,j}
\mu_{ij}
\fc{|A\cap A_i||B\cap B_j|}{|V|^2}.\]</div>
<p>This is the weak regularity of Frieze-Kannan. For Szemeredi we need
the stronger boosting lemma (see previous point).</p>
<p>We can also do something similar with <span class="math">\(G\)</span> a subset of an
expander. The expander mixing lemma gives an error term.</p>
</li>
</ol>
<table class="docutils footnote" frame="void" id="id1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>When <span class="math">\(U\)</span> is not uniform and has distribution <span class="math">\(u(x)\)</span>, this
is <span class="math">\(\fc{\mu(x)u(x)}{\sum_{x'\in U} \mu(x')u(x')}\)</span>.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[2]</td><td>We ignore sample complexity here. In reality, because we only see
<span class="math">\(U\)</span> from samples, we need to think about generalization. If the
VC-dimension of <span class="math">\(\mathcal T\)</span> is <span class="math">\(d\)</span>, then the
VC-dimension of <span class="math">\(\mathcal F_k\mathcal H\)</span> is at most
<span class="math">\(k^d\)</span>. In ML we don’t want to take <span class="math">\(\mathcal F\)</span> to be the
class of all boolean functions. For this theorem, let’s just assume
we are actually given all pairs <span class="math">\((x,f(x))\)</span>.</td></tr>
</tbody>
</table>
</div>


          </div>
        </div>
      </div>
    <div class="clearer"></div>
  </div>
    <div class="footer">
      &copy;2017, Simons Institute.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.5.3</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.9</a>
      
      |
      <a href="../_sources/groups/1.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>